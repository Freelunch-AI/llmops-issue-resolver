{"instance_id": "sympy__sympy-19954", "model_patch": "diff --git a/experimentation/SWE-bench b/experimentation/SWE-bench\n--- a/experimentation/SWE-bench\n+++ b/experimentation/SWE-bench\n@@ -1 +1 @@\n-Subproject commit 5f5a7df799663adba4b191eca3d675faf3621fe2\n+Subproject commit 5f5a7df799663adba4b191eca3d675faf3621fe2-dirty\ndiff --git a/experimentation/code/scripts/generate_inferences.py b/experimentation/code/scripts/generate_inferences.py\nindex 0b9a2bb..31297ec 100755\n--- a/experimentation/code/scripts/generate_inferences.py\n+++ b/experimentation/code/scripts/generate_inferences.py\n@@ -7,9 +7,9 @@ import sys\n from contextlib import contextmanager\n from typing import List\n \n-import datasets as ds\n import pandas as pd\n import yaml\n+from datasets import load_dataset\n from pydantic import ValidationError\n \n \n@@ -89,23 +89,23 @@ def get_instances_ids(dataset_pointer_path: str, dataset_name: str, \\\n     with open(dataset_pointer_path, \"r\") as f:\n         datasets = yaml.safe_load(f)\n \n-    dataset_hf_path = datasets[dataset_name][\"url\"]\n-    print(f\"Dataset hugging face path: {dataset_hf_path}\")\n+    dataset_hf_name= datasets[dataset_name][\"hf_name\"]\n+    print(f\"dataset_hf_name: {dataset_hf_name}\")\n \n-    # Load the dataset from the the huggin face dataset path\n-    dataset = ds.load_dataset(dataset_hf_path, name=dataset_name)\n+    # Load the dataset from the the hugging face dataset path\n+    dataset = load_dataset(dataset_hf_name)\n \n     # Store the dataset locally as parquet\n     dataset_local_path = f\"datasets/{dataset_name}/{dataset_name}.parquet\"\n     os.makedirs(os.path.dirname(dataset_local_path), exist_ok=True)\n-    dataset.to_pandas().to_parquet(dataset_local_path)\n+    dataset[\"test\"].to_pandas().to_parquet(dataset_local_path)\n \n     # Get the instance IDs\n     if random_sampling:\n-        instances_ids = dataset['train'].shuffle(DEFAULT_SEED).select( \\\n+        instances_ids = dataset[\"test\"].shuffle(DEFAULT_SEED).select( \\\n                         range(number_of_instances))['instance_id']\n     else:\n-        instances_ids = dataset['train'].select(range(number_of_instances)) \\\n+        instances_ids = dataset[\"test\"].select(range(number_of_instances)) \\\n                                                 ['instance_id']\n \n     return instances_ids\n@@ -149,7 +149,7 @@ def setup_instance(dataset_name:str, instance_id: str):\n         raise\n \n     # Load dataset information from datasets.yml\n-    with open(\"/datasets/datasets.yml\", \"r\") as f:\n+    with open(\"datasets/datasets.yml\", \"r\") as f:\n         datasets = yaml.safe_load(f)\n \n     dataset_info = datasets.get(dataset_name)\n@@ -162,17 +162,17 @@ def setup_instance(dataset_name:str, instance_id: str):\n     if not os.path.exists(dataset_path):\n         raise ValueError(f\"Dataset file not found at {dataset_path}\")\n \n-    dataset = pd.read_parquet(dataset_path).to_dict(orient=\"records\")\n+    dataset = pd.read_parquet(dataset_path)\n \n-    # Find the instance by instance_id\n-    instance = next((item for item in dataset if item[\"instance_id\"] == instance_id), \n-                    None)\n-    if not instance:\n+    # Find the instance by instance_id \n+    instance = dataset[dataset[\"instance_id\"] == instance_id].iloc[0]\n+\n+    if instance.empty:\n         raise ValueError(f\"Instance {instance_id} not found in dataset\")\n \n     # Pull the repo at the specific commit\n     base_commit = instance[\"base_commit\"]\n-    os.system(f\"git clone {dataset_info['repo_url']} repo\")\n+    os.system(f\"git clone https://github.com/{instance['repo']}.git repo\")\n     os.chdir(\"repo\")\n     os.system(f\"git checkout {base_commit}\")\n \n@@ -292,9 +292,9 @@ def main() -> None:\n         experiment_name, skipped_instance = run_ai()\n         if not skipped_instance:\n             append_to_inferences(inferences_path=os.path.join(\"results\", \n-                                os.path.join(\"subresults\", \"inference.jsonl\"), \\\n+                                os.path.join(\"subresults\", \"inference.jsonl\")), \\\n                                 instance_id=instance_id, \\\n-                                experiment_name=experiment_name))\n+                                experiment_name=experiment_name)\n         else:\n             skipped_instances += 1\n     print(skipped_instances)\ndiff --git a/experimentation/datasets/datasets.yml b/experimentation/datasets/datasets.yml\nindex 8bf08d0..d2f5984 100755\n--- a/experimentation/datasets/datasets.yml\n+++ b/experimentation/datasets/datasets.yml\n@@ -1,3 +1,3 @@\n swe_bench_verified: \n-  url: https://huggingface.co/datasets/princeton-nlp/SWE-bench_Verified\n+  hf_name: princeton-nlp/SWE-bench_Verified\n   local_path: datasets/swe_verified/swe_verified.parquet\n\\ No newline at end of file\ndiff --git a/src/llmops_issue_resolver/cli.py b/src/llmops_issue_resolver/cli.py\nindex 51072c6..37f821e 100644\n--- a/src/llmops_issue_resolver/cli.py\n+++ b/src/llmops_issue_resolver/cli.py\n@@ -36,7 +36,7 @@ def temporary_sys_path(path):\n #     tree\n # All .py modules need to have this line, but with the more general form of the import \n \n-with temporary_sys_path(os.path.abspath(os.join(\"../\", os.path.dirname(__file__)))):\n+with temporary_sys_path(os.path.abspath(os.join(os.path.dirname(__file__), \"../\"))):\n     from llmops_issue_resolver.agent import graph\n \n app = typer.Typer()\n", "model_name_or_path": "placeholder__placeholder__placeholder__placeholder__placeholder"}
{"instance_id": "django__django-13821", "model_patch": "diff --git a/experimentation/SWE-bench b/experimentation/SWE-bench\n--- a/experimentation/SWE-bench\n+++ b/experimentation/SWE-bench\n@@ -1 +1 @@\n-Subproject commit 5f5a7df799663adba4b191eca3d675faf3621fe2\n+Subproject commit 5f5a7df799663adba4b191eca3d675faf3621fe2-dirty\ndiff --git a/experimentation/code/scripts/generate_inferences.py b/experimentation/code/scripts/generate_inferences.py\nindex 0b9a2bb..31297ec 100755\n--- a/experimentation/code/scripts/generate_inferences.py\n+++ b/experimentation/code/scripts/generate_inferences.py\n@@ -7,9 +7,9 @@ import sys\n from contextlib import contextmanager\n from typing import List\n \n-import datasets as ds\n import pandas as pd\n import yaml\n+from datasets import load_dataset\n from pydantic import ValidationError\n \n \n@@ -89,23 +89,23 @@ def get_instances_ids(dataset_pointer_path: str, dataset_name: str, \\\n     with open(dataset_pointer_path, \"r\") as f:\n         datasets = yaml.safe_load(f)\n \n-    dataset_hf_path = datasets[dataset_name][\"url\"]\n-    print(f\"Dataset hugging face path: {dataset_hf_path}\")\n+    dataset_hf_name= datasets[dataset_name][\"hf_name\"]\n+    print(f\"dataset_hf_name: {dataset_hf_name}\")\n \n-    # Load the dataset from the the huggin face dataset path\n-    dataset = ds.load_dataset(dataset_hf_path, name=dataset_name)\n+    # Load the dataset from the the hugging face dataset path\n+    dataset = load_dataset(dataset_hf_name)\n \n     # Store the dataset locally as parquet\n     dataset_local_path = f\"datasets/{dataset_name}/{dataset_name}.parquet\"\n     os.makedirs(os.path.dirname(dataset_local_path), exist_ok=True)\n-    dataset.to_pandas().to_parquet(dataset_local_path)\n+    dataset[\"test\"].to_pandas().to_parquet(dataset_local_path)\n \n     # Get the instance IDs\n     if random_sampling:\n-        instances_ids = dataset['train'].shuffle(DEFAULT_SEED).select( \\\n+        instances_ids = dataset[\"test\"].shuffle(DEFAULT_SEED).select( \\\n                         range(number_of_instances))['instance_id']\n     else:\n-        instances_ids = dataset['train'].select(range(number_of_instances)) \\\n+        instances_ids = dataset[\"test\"].select(range(number_of_instances)) \\\n                                                 ['instance_id']\n \n     return instances_ids\n@@ -149,7 +149,7 @@ def setup_instance(dataset_name:str, instance_id: str):\n         raise\n \n     # Load dataset information from datasets.yml\n-    with open(\"/datasets/datasets.yml\", \"r\") as f:\n+    with open(\"datasets/datasets.yml\", \"r\") as f:\n         datasets = yaml.safe_load(f)\n \n     dataset_info = datasets.get(dataset_name)\n@@ -162,17 +162,17 @@ def setup_instance(dataset_name:str, instance_id: str):\n     if not os.path.exists(dataset_path):\n         raise ValueError(f\"Dataset file not found at {dataset_path}\")\n \n-    dataset = pd.read_parquet(dataset_path).to_dict(orient=\"records\")\n+    dataset = pd.read_parquet(dataset_path)\n \n-    # Find the instance by instance_id\n-    instance = next((item for item in dataset if item[\"instance_id\"] == instance_id), \n-                    None)\n-    if not instance:\n+    # Find the instance by instance_id \n+    instance = dataset[dataset[\"instance_id\"] == instance_id].iloc[0]\n+\n+    if instance.empty:\n         raise ValueError(f\"Instance {instance_id} not found in dataset\")\n \n     # Pull the repo at the specific commit\n     base_commit = instance[\"base_commit\"]\n-    os.system(f\"git clone {dataset_info['repo_url']} repo\")\n+    os.system(f\"git clone https://github.com/{instance['repo']}.git repo\")\n     os.chdir(\"repo\")\n     os.system(f\"git checkout {base_commit}\")\n \n@@ -292,9 +292,9 @@ def main() -> None:\n         experiment_name, skipped_instance = run_ai()\n         if not skipped_instance:\n             append_to_inferences(inferences_path=os.path.join(\"results\", \n-                                os.path.join(\"subresults\", \"inference.jsonl\"), \\\n+                                os.path.join(\"subresults\", \"inference.jsonl\")), \\\n                                 instance_id=instance_id, \\\n-                                experiment_name=experiment_name))\n+                                experiment_name=experiment_name)\n         else:\n             skipped_instances += 1\n     print(skipped_instances)\ndiff --git a/experimentation/datasets/datasets.yml b/experimentation/datasets/datasets.yml\nindex 8bf08d0..d2f5984 100755\n--- a/experimentation/datasets/datasets.yml\n+++ b/experimentation/datasets/datasets.yml\n@@ -1,3 +1,3 @@\n swe_bench_verified: \n-  url: https://huggingface.co/datasets/princeton-nlp/SWE-bench_Verified\n+  hf_name: princeton-nlp/SWE-bench_Verified\n   local_path: datasets/swe_verified/swe_verified.parquet\n\\ No newline at end of file\ndiff --git a/src/llmops_issue_resolver/cli.py b/src/llmops_issue_resolver/cli.py\nindex 51072c6..37f821e 100644\n--- a/src/llmops_issue_resolver/cli.py\n+++ b/src/llmops_issue_resolver/cli.py\n@@ -36,7 +36,7 @@ def temporary_sys_path(path):\n #     tree\n # All .py modules need to have this line, but with the more general form of the import \n \n-with temporary_sys_path(os.path.abspath(os.join(\"../\", os.path.dirname(__file__)))):\n+with temporary_sys_path(os.path.abspath(os.join(os.path.dirname(__file__), \"../\"))):\n     from llmops_issue_resolver.agent import graph\n \n app = typer.Typer()\n", "model_name_or_path": "placeholder__placeholder__placeholder__placeholder__placeholder"}
{"instance_id": "sympy__sympy-19783", "model_patch": "diff --git a/experimentation/SWE-bench b/experimentation/SWE-bench\n--- a/experimentation/SWE-bench\n+++ b/experimentation/SWE-bench\n@@ -1 +1 @@\n-Subproject commit 5f5a7df799663adba4b191eca3d675faf3621fe2\n+Subproject commit 5f5a7df799663adba4b191eca3d675faf3621fe2-dirty\ndiff --git a/experimentation/code/scripts/generate_inferences.py b/experimentation/code/scripts/generate_inferences.py\nindex 0b9a2bb..31297ec 100755\n--- a/experimentation/code/scripts/generate_inferences.py\n+++ b/experimentation/code/scripts/generate_inferences.py\n@@ -7,9 +7,9 @@ import sys\n from contextlib import contextmanager\n from typing import List\n \n-import datasets as ds\n import pandas as pd\n import yaml\n+from datasets import load_dataset\n from pydantic import ValidationError\n \n \n@@ -89,23 +89,23 @@ def get_instances_ids(dataset_pointer_path: str, dataset_name: str, \\\n     with open(dataset_pointer_path, \"r\") as f:\n         datasets = yaml.safe_load(f)\n \n-    dataset_hf_path = datasets[dataset_name][\"url\"]\n-    print(f\"Dataset hugging face path: {dataset_hf_path}\")\n+    dataset_hf_name= datasets[dataset_name][\"hf_name\"]\n+    print(f\"dataset_hf_name: {dataset_hf_name}\")\n \n-    # Load the dataset from the the huggin face dataset path\n-    dataset = ds.load_dataset(dataset_hf_path, name=dataset_name)\n+    # Load the dataset from the the hugging face dataset path\n+    dataset = load_dataset(dataset_hf_name)\n \n     # Store the dataset locally as parquet\n     dataset_local_path = f\"datasets/{dataset_name}/{dataset_name}.parquet\"\n     os.makedirs(os.path.dirname(dataset_local_path), exist_ok=True)\n-    dataset.to_pandas().to_parquet(dataset_local_path)\n+    dataset[\"test\"].to_pandas().to_parquet(dataset_local_path)\n \n     # Get the instance IDs\n     if random_sampling:\n-        instances_ids = dataset['train'].shuffle(DEFAULT_SEED).select( \\\n+        instances_ids = dataset[\"test\"].shuffle(DEFAULT_SEED).select( \\\n                         range(number_of_instances))['instance_id']\n     else:\n-        instances_ids = dataset['train'].select(range(number_of_instances)) \\\n+        instances_ids = dataset[\"test\"].select(range(number_of_instances)) \\\n                                                 ['instance_id']\n \n     return instances_ids\n@@ -149,7 +149,7 @@ def setup_instance(dataset_name:str, instance_id: str):\n         raise\n \n     # Load dataset information from datasets.yml\n-    with open(\"/datasets/datasets.yml\", \"r\") as f:\n+    with open(\"datasets/datasets.yml\", \"r\") as f:\n         datasets = yaml.safe_load(f)\n \n     dataset_info = datasets.get(dataset_name)\n@@ -162,17 +162,17 @@ def setup_instance(dataset_name:str, instance_id: str):\n     if not os.path.exists(dataset_path):\n         raise ValueError(f\"Dataset file not found at {dataset_path}\")\n \n-    dataset = pd.read_parquet(dataset_path).to_dict(orient=\"records\")\n+    dataset = pd.read_parquet(dataset_path)\n \n-    # Find the instance by instance_id\n-    instance = next((item for item in dataset if item[\"instance_id\"] == instance_id), \n-                    None)\n-    if not instance:\n+    # Find the instance by instance_id \n+    instance = dataset[dataset[\"instance_id\"] == instance_id].iloc[0]\n+\n+    if instance.empty:\n         raise ValueError(f\"Instance {instance_id} not found in dataset\")\n \n     # Pull the repo at the specific commit\n     base_commit = instance[\"base_commit\"]\n-    os.system(f\"git clone {dataset_info['repo_url']} repo\")\n+    os.system(f\"git clone https://github.com/{instance['repo']}.git repo\")\n     os.chdir(\"repo\")\n     os.system(f\"git checkout {base_commit}\")\n \n@@ -292,9 +292,9 @@ def main() -> None:\n         experiment_name, skipped_instance = run_ai()\n         if not skipped_instance:\n             append_to_inferences(inferences_path=os.path.join(\"results\", \n-                                os.path.join(\"subresults\", \"inference.jsonl\"), \\\n+                                os.path.join(\"subresults\", \"inference.jsonl\")), \\\n                                 instance_id=instance_id, \\\n-                                experiment_name=experiment_name))\n+                                experiment_name=experiment_name)\n         else:\n             skipped_instances += 1\n     print(skipped_instances)\ndiff --git a/experimentation/datasets/datasets.yml b/experimentation/datasets/datasets.yml\nindex 8bf08d0..d2f5984 100755\n--- a/experimentation/datasets/datasets.yml\n+++ b/experimentation/datasets/datasets.yml\n@@ -1,3 +1,3 @@\n swe_bench_verified: \n-  url: https://huggingface.co/datasets/princeton-nlp/SWE-bench_Verified\n+  hf_name: princeton-nlp/SWE-bench_Verified\n   local_path: datasets/swe_verified/swe_verified.parquet\n\\ No newline at end of file\ndiff --git a/src/llmops_issue_resolver/cli.py b/src/llmops_issue_resolver/cli.py\nindex 51072c6..37f821e 100644\n--- a/src/llmops_issue_resolver/cli.py\n+++ b/src/llmops_issue_resolver/cli.py\n@@ -36,7 +36,7 @@ def temporary_sys_path(path):\n #     tree\n # All .py modules need to have this line, but with the more general form of the import \n \n-with temporary_sys_path(os.path.abspath(os.join(\"../\", os.path.dirname(__file__)))):\n+with temporary_sys_path(os.path.abspath(os.join(os.path.dirname(__file__), \"../\"))):\n     from llmops_issue_resolver.agent import graph\n \n app = typer.Typer()\n", "model_name_or_path": "placeholder__placeholder__placeholder__placeholder__placeholder"}
{"instance_id": "django__django-11333", "model_patch": "diff --git a/experimentation/SWE-bench b/experimentation/SWE-bench\n--- a/experimentation/SWE-bench\n+++ b/experimentation/SWE-bench\n@@ -1 +1 @@\n-Subproject commit 5f5a7df799663adba4b191eca3d675faf3621fe2\n+Subproject commit 5f5a7df799663adba4b191eca3d675faf3621fe2-dirty\ndiff --git a/experimentation/code/scripts/generate_inferences.py b/experimentation/code/scripts/generate_inferences.py\nindex 0b9a2bb..31297ec 100755\n--- a/experimentation/code/scripts/generate_inferences.py\n+++ b/experimentation/code/scripts/generate_inferences.py\n@@ -7,9 +7,9 @@ import sys\n from contextlib import contextmanager\n from typing import List\n \n-import datasets as ds\n import pandas as pd\n import yaml\n+from datasets import load_dataset\n from pydantic import ValidationError\n \n \n@@ -89,23 +89,23 @@ def get_instances_ids(dataset_pointer_path: str, dataset_name: str, \\\n     with open(dataset_pointer_path, \"r\") as f:\n         datasets = yaml.safe_load(f)\n \n-    dataset_hf_path = datasets[dataset_name][\"url\"]\n-    print(f\"Dataset hugging face path: {dataset_hf_path}\")\n+    dataset_hf_name= datasets[dataset_name][\"hf_name\"]\n+    print(f\"dataset_hf_name: {dataset_hf_name}\")\n \n-    # Load the dataset from the the huggin face dataset path\n-    dataset = ds.load_dataset(dataset_hf_path, name=dataset_name)\n+    # Load the dataset from the the hugging face dataset path\n+    dataset = load_dataset(dataset_hf_name)\n \n     # Store the dataset locally as parquet\n     dataset_local_path = f\"datasets/{dataset_name}/{dataset_name}.parquet\"\n     os.makedirs(os.path.dirname(dataset_local_path), exist_ok=True)\n-    dataset.to_pandas().to_parquet(dataset_local_path)\n+    dataset[\"test\"].to_pandas().to_parquet(dataset_local_path)\n \n     # Get the instance IDs\n     if random_sampling:\n-        instances_ids = dataset['train'].shuffle(DEFAULT_SEED).select( \\\n+        instances_ids = dataset[\"test\"].shuffle(DEFAULT_SEED).select( \\\n                         range(number_of_instances))['instance_id']\n     else:\n-        instances_ids = dataset['train'].select(range(number_of_instances)) \\\n+        instances_ids = dataset[\"test\"].select(range(number_of_instances)) \\\n                                                 ['instance_id']\n \n     return instances_ids\n@@ -149,7 +149,7 @@ def setup_instance(dataset_name:str, instance_id: str):\n         raise\n \n     # Load dataset information from datasets.yml\n-    with open(\"/datasets/datasets.yml\", \"r\") as f:\n+    with open(\"datasets/datasets.yml\", \"r\") as f:\n         datasets = yaml.safe_load(f)\n \n     dataset_info = datasets.get(dataset_name)\n@@ -162,17 +162,17 @@ def setup_instance(dataset_name:str, instance_id: str):\n     if not os.path.exists(dataset_path):\n         raise ValueError(f\"Dataset file not found at {dataset_path}\")\n \n-    dataset = pd.read_parquet(dataset_path).to_dict(orient=\"records\")\n+    dataset = pd.read_parquet(dataset_path)\n \n-    # Find the instance by instance_id\n-    instance = next((item for item in dataset if item[\"instance_id\"] == instance_id), \n-                    None)\n-    if not instance:\n+    # Find the instance by instance_id \n+    instance = dataset[dataset[\"instance_id\"] == instance_id].iloc[0]\n+\n+    if instance.empty:\n         raise ValueError(f\"Instance {instance_id} not found in dataset\")\n \n     # Pull the repo at the specific commit\n     base_commit = instance[\"base_commit\"]\n-    os.system(f\"git clone {dataset_info['repo_url']} repo\")\n+    os.system(f\"git clone https://github.com/{instance['repo']}.git repo\")\n     os.chdir(\"repo\")\n     os.system(f\"git checkout {base_commit}\")\n \n@@ -292,9 +292,9 @@ def main() -> None:\n         experiment_name, skipped_instance = run_ai()\n         if not skipped_instance:\n             append_to_inferences(inferences_path=os.path.join(\"results\", \n-                                os.path.join(\"subresults\", \"inference.jsonl\"), \\\n+                                os.path.join(\"subresults\", \"inference.jsonl\")), \\\n                                 instance_id=instance_id, \\\n-                                experiment_name=experiment_name))\n+                                experiment_name=experiment_name)\n         else:\n             skipped_instances += 1\n     print(skipped_instances)\ndiff --git a/experimentation/datasets/datasets.yml b/experimentation/datasets/datasets.yml\nindex 8bf08d0..d2f5984 100755\n--- a/experimentation/datasets/datasets.yml\n+++ b/experimentation/datasets/datasets.yml\n@@ -1,3 +1,3 @@\n swe_bench_verified: \n-  url: https://huggingface.co/datasets/princeton-nlp/SWE-bench_Verified\n+  hf_name: princeton-nlp/SWE-bench_Verified\n   local_path: datasets/swe_verified/swe_verified.parquet\n\\ No newline at end of file\ndiff --git a/src/llmops_issue_resolver/cli.py b/src/llmops_issue_resolver/cli.py\nindex 51072c6..37f821e 100644\n--- a/src/llmops_issue_resolver/cli.py\n+++ b/src/llmops_issue_resolver/cli.py\n@@ -36,7 +36,7 @@ def temporary_sys_path(path):\n #     tree\n # All .py modules need to have this line, but with the more general form of the import \n \n-with temporary_sys_path(os.path.abspath(os.join(\"../\", os.path.dirname(__file__)))):\n+with temporary_sys_path(os.path.abspath(os.join(os.path.dirname(__file__), \"../\"))):\n     from llmops_issue_resolver.agent import graph\n \n app = typer.Typer()\n", "model_name_or_path": "placeholder__placeholder__placeholder__placeholder__placeholder"}
{"instance_id": "django__django-11211", "model_patch": "diff --git a/experimentation/SWE-bench b/experimentation/SWE-bench\n--- a/experimentation/SWE-bench\n+++ b/experimentation/SWE-bench\n@@ -1 +1 @@\n-Subproject commit 5f5a7df799663adba4b191eca3d675faf3621fe2\n+Subproject commit 5f5a7df799663adba4b191eca3d675faf3621fe2-dirty\ndiff --git a/experimentation/code/scripts/generate_inferences.py b/experimentation/code/scripts/generate_inferences.py\nindex 0b9a2bb..31297ec 100755\n--- a/experimentation/code/scripts/generate_inferences.py\n+++ b/experimentation/code/scripts/generate_inferences.py\n@@ -7,9 +7,9 @@ import sys\n from contextlib import contextmanager\n from typing import List\n \n-import datasets as ds\n import pandas as pd\n import yaml\n+from datasets import load_dataset\n from pydantic import ValidationError\n \n \n@@ -89,23 +89,23 @@ def get_instances_ids(dataset_pointer_path: str, dataset_name: str, \\\n     with open(dataset_pointer_path, \"r\") as f:\n         datasets = yaml.safe_load(f)\n \n-    dataset_hf_path = datasets[dataset_name][\"url\"]\n-    print(f\"Dataset hugging face path: {dataset_hf_path}\")\n+    dataset_hf_name= datasets[dataset_name][\"hf_name\"]\n+    print(f\"dataset_hf_name: {dataset_hf_name}\")\n \n-    # Load the dataset from the the huggin face dataset path\n-    dataset = ds.load_dataset(dataset_hf_path, name=dataset_name)\n+    # Load the dataset from the the hugging face dataset path\n+    dataset = load_dataset(dataset_hf_name)\n \n     # Store the dataset locally as parquet\n     dataset_local_path = f\"datasets/{dataset_name}/{dataset_name}.parquet\"\n     os.makedirs(os.path.dirname(dataset_local_path), exist_ok=True)\n-    dataset.to_pandas().to_parquet(dataset_local_path)\n+    dataset[\"test\"].to_pandas().to_parquet(dataset_local_path)\n \n     # Get the instance IDs\n     if random_sampling:\n-        instances_ids = dataset['train'].shuffle(DEFAULT_SEED).select( \\\n+        instances_ids = dataset[\"test\"].shuffle(DEFAULT_SEED).select( \\\n                         range(number_of_instances))['instance_id']\n     else:\n-        instances_ids = dataset['train'].select(range(number_of_instances)) \\\n+        instances_ids = dataset[\"test\"].select(range(number_of_instances)) \\\n                                                 ['instance_id']\n \n     return instances_ids\n@@ -149,7 +149,7 @@ def setup_instance(dataset_name:str, instance_id: str):\n         raise\n \n     # Load dataset information from datasets.yml\n-    with open(\"/datasets/datasets.yml\", \"r\") as f:\n+    with open(\"datasets/datasets.yml\", \"r\") as f:\n         datasets = yaml.safe_load(f)\n \n     dataset_info = datasets.get(dataset_name)\n@@ -162,17 +162,17 @@ def setup_instance(dataset_name:str, instance_id: str):\n     if not os.path.exists(dataset_path):\n         raise ValueError(f\"Dataset file not found at {dataset_path}\")\n \n-    dataset = pd.read_parquet(dataset_path).to_dict(orient=\"records\")\n+    dataset = pd.read_parquet(dataset_path)\n \n-    # Find the instance by instance_id\n-    instance = next((item for item in dataset if item[\"instance_id\"] == instance_id), \n-                    None)\n-    if not instance:\n+    # Find the instance by instance_id \n+    instance = dataset[dataset[\"instance_id\"] == instance_id].iloc[0]\n+\n+    if instance.empty:\n         raise ValueError(f\"Instance {instance_id} not found in dataset\")\n \n     # Pull the repo at the specific commit\n     base_commit = instance[\"base_commit\"]\n-    os.system(f\"git clone {dataset_info['repo_url']} repo\")\n+    os.system(f\"git clone https://github.com/{instance['repo']}.git repo\")\n     os.chdir(\"repo\")\n     os.system(f\"git checkout {base_commit}\")\n \n@@ -292,9 +292,9 @@ def main() -> None:\n         experiment_name, skipped_instance = run_ai()\n         if not skipped_instance:\n             append_to_inferences(inferences_path=os.path.join(\"results\", \n-                                os.path.join(\"subresults\", \"inference.jsonl\"), \\\n+                                os.path.join(\"subresults\", \"inference.jsonl\")), \\\n                                 instance_id=instance_id, \\\n-                                experiment_name=experiment_name))\n+                                experiment_name=experiment_name)\n         else:\n             skipped_instances += 1\n     print(skipped_instances)\ndiff --git a/experimentation/datasets/datasets.yml b/experimentation/datasets/datasets.yml\nindex 8bf08d0..d2f5984 100755\n--- a/experimentation/datasets/datasets.yml\n+++ b/experimentation/datasets/datasets.yml\n@@ -1,3 +1,3 @@\n swe_bench_verified: \n-  url: https://huggingface.co/datasets/princeton-nlp/SWE-bench_Verified\n+  hf_name: princeton-nlp/SWE-bench_Verified\n   local_path: datasets/swe_verified/swe_verified.parquet\n\\ No newline at end of file\ndiff --git a/src/llmops_issue_resolver/cli.py b/src/llmops_issue_resolver/cli.py\nindex 51072c6..37f821e 100644\n--- a/src/llmops_issue_resolver/cli.py\n+++ b/src/llmops_issue_resolver/cli.py\n@@ -36,7 +36,7 @@ def temporary_sys_path(path):\n #     tree\n # All .py modules need to have this line, but with the more general form of the import \n \n-with temporary_sys_path(os.path.abspath(os.join(\"../\", os.path.dirname(__file__)))):\n+with temporary_sys_path(os.path.abspath(os.join(os.path.dirname(__file__), \"../\"))):\n     from llmops_issue_resolver.agent import graph\n \n app = typer.Typer()\n", "model_name_or_path": "placeholder__placeholder__placeholder__placeholder__placeholder"}
{"instance_id": "sympy__sympy-19954", "model_patch": "diff --git a/experimentation/SWE-bench b/experimentation/SWE-bench\n--- a/experimentation/SWE-bench\n+++ b/experimentation/SWE-bench\n@@ -1 +1 @@\n-Subproject commit 5f5a7df799663adba4b191eca3d675faf3621fe2\n+Subproject commit 5f5a7df799663adba4b191eca3d675faf3621fe2-dirty\ndiff --git a/experimentation/code/scripts/generate_inferences.py b/experimentation/code/scripts/generate_inferences.py\nindex 0b9a2bb..27c731e 100755\n--- a/experimentation/code/scripts/generate_inferences.py\n+++ b/experimentation/code/scripts/generate_inferences.py\n@@ -3,13 +3,14 @@\n import argparse\n import json\n import os\n+import shutil\n import sys\n from contextlib import contextmanager\n from typing import List\n \n-import datasets as ds\n import pandas as pd\n import yaml\n+from datasets import load_dataset\n from pydantic import ValidationError\n \n \n@@ -89,23 +90,23 @@ def get_instances_ids(dataset_pointer_path: str, dataset_name: str, \\\n     with open(dataset_pointer_path, \"r\") as f:\n         datasets = yaml.safe_load(f)\n \n-    dataset_hf_path = datasets[dataset_name][\"url\"]\n-    print(f\"Dataset hugging face path: {dataset_hf_path}\")\n+    dataset_hf_name= datasets[dataset_name][\"hf_name\"]\n+    print(f\"dataset_hf_name: {dataset_hf_name}\")\n \n-    # Load the dataset from the the huggin face dataset path\n-    dataset = ds.load_dataset(dataset_hf_path, name=dataset_name)\n+    # Load the dataset from the the hugging face dataset path\n+    dataset = load_dataset(dataset_hf_name)\n \n     # Store the dataset locally as parquet\n     dataset_local_path = f\"datasets/{dataset_name}/{dataset_name}.parquet\"\n     os.makedirs(os.path.dirname(dataset_local_path), exist_ok=True)\n-    dataset.to_pandas().to_parquet(dataset_local_path)\n+    dataset[\"test\"].to_pandas().to_parquet(dataset_local_path)\n \n     # Get the instance IDs\n     if random_sampling:\n-        instances_ids = dataset['train'].shuffle(DEFAULT_SEED).select( \\\n+        instances_ids = dataset[\"test\"].shuffle(DEFAULT_SEED).select( \\\n                         range(number_of_instances))['instance_id']\n     else:\n-        instances_ids = dataset['train'].select(range(number_of_instances)) \\\n+        instances_ids = dataset[\"test\"].select(range(number_of_instances)) \\\n                                                 ['instance_id']\n \n     return instances_ids\n@@ -149,7 +150,7 @@ def setup_instance(dataset_name:str, instance_id: str):\n         raise\n \n     # Load dataset information from datasets.yml\n-    with open(\"/datasets/datasets.yml\", \"r\") as f:\n+    with open(\"datasets/datasets.yml\", \"r\") as f:\n         datasets = yaml.safe_load(f)\n \n     dataset_info = datasets.get(dataset_name)\n@@ -162,17 +163,20 @@ def setup_instance(dataset_name:str, instance_id: str):\n     if not os.path.exists(dataset_path):\n         raise ValueError(f\"Dataset file not found at {dataset_path}\")\n \n-    dataset = pd.read_parquet(dataset_path).to_dict(orient=\"records\")\n+    dataset = pd.read_parquet(dataset_path)\n \n-    # Find the instance by instance_id\n-    instance = next((item for item in dataset if item[\"instance_id\"] == instance_id), \n-                    None)\n-    if not instance:\n+    # Find the instance by instance_id \n+    instance = dataset[dataset[\"instance_id\"] == instance_id].iloc[0]\n+\n+    if instance.empty:\n         raise ValueError(f\"Instance {instance_id} not found in dataset\")\n \n     # Pull the repo at the specific commit\n     base_commit = instance[\"base_commit\"]\n-    os.system(f\"git clone {dataset_info['repo_url']} repo\")\n+    # empty the repo directory\n+    shutil.rmtree(\"repo\")\n+    # clone the repo\n+    os.system(f\"git clone https://github.com/{instance['repo']}.git repo\")\n     os.chdir(\"repo\")\n     os.system(f\"git checkout {base_commit}\")\n \n@@ -292,9 +296,9 @@ def main() -> None:\n         experiment_name, skipped_instance = run_ai()\n         if not skipped_instance:\n             append_to_inferences(inferences_path=os.path.join(\"results\", \n-                                os.path.join(\"subresults\", \"inference.jsonl\"), \\\n+                                os.path.join(\"subresults\", \"inference.jsonl\")), \\\n                                 instance_id=instance_id, \\\n-                                experiment_name=experiment_name))\n+                                experiment_name=experiment_name)\n         else:\n             skipped_instances += 1\n     print(skipped_instances)\ndiff --git a/experimentation/datasets/datasets.yml b/experimentation/datasets/datasets.yml\nindex 8bf08d0..d2f5984 100755\n--- a/experimentation/datasets/datasets.yml\n+++ b/experimentation/datasets/datasets.yml\n@@ -1,3 +1,3 @@\n swe_bench_verified: \n-  url: https://huggingface.co/datasets/princeton-nlp/SWE-bench_Verified\n+  hf_name: princeton-nlp/SWE-bench_Verified\n   local_path: datasets/swe_verified/swe_verified.parquet\n\\ No newline at end of file\ndiff --git a/src/llmops_issue_resolver/cli.py b/src/llmops_issue_resolver/cli.py\nindex 51072c6..37f821e 100644\n--- a/src/llmops_issue_resolver/cli.py\n+++ b/src/llmops_issue_resolver/cli.py\n@@ -36,7 +36,7 @@ def temporary_sys_path(path):\n #     tree\n # All .py modules need to have this line, but with the more general form of the import \n \n-with temporary_sys_path(os.path.abspath(os.join(\"../\", os.path.dirname(__file__)))):\n+with temporary_sys_path(os.path.abspath(os.join(os.path.dirname(__file__), \"../\"))):\n     from llmops_issue_resolver.agent import graph\n \n app = typer.Typer()\n", "model_name_or_path": "placeholder__placeholder__placeholder__placeholder__placeholder"}
{"instance_id": "django__django-13821", "model_patch": "diff --git a/experimentation/SWE-bench b/experimentation/SWE-bench\n--- a/experimentation/SWE-bench\n+++ b/experimentation/SWE-bench\n@@ -1 +1 @@\n-Subproject commit 5f5a7df799663adba4b191eca3d675faf3621fe2\n+Subproject commit 5f5a7df799663adba4b191eca3d675faf3621fe2-dirty\ndiff --git a/experimentation/code/scripts/generate_inferences.py b/experimentation/code/scripts/generate_inferences.py\nindex 0b9a2bb..27c731e 100755\n--- a/experimentation/code/scripts/generate_inferences.py\n+++ b/experimentation/code/scripts/generate_inferences.py\n@@ -3,13 +3,14 @@\n import argparse\n import json\n import os\n+import shutil\n import sys\n from contextlib import contextmanager\n from typing import List\n \n-import datasets as ds\n import pandas as pd\n import yaml\n+from datasets import load_dataset\n from pydantic import ValidationError\n \n \n@@ -89,23 +90,23 @@ def get_instances_ids(dataset_pointer_path: str, dataset_name: str, \\\n     with open(dataset_pointer_path, \"r\") as f:\n         datasets = yaml.safe_load(f)\n \n-    dataset_hf_path = datasets[dataset_name][\"url\"]\n-    print(f\"Dataset hugging face path: {dataset_hf_path}\")\n+    dataset_hf_name= datasets[dataset_name][\"hf_name\"]\n+    print(f\"dataset_hf_name: {dataset_hf_name}\")\n \n-    # Load the dataset from the the huggin face dataset path\n-    dataset = ds.load_dataset(dataset_hf_path, name=dataset_name)\n+    # Load the dataset from the the hugging face dataset path\n+    dataset = load_dataset(dataset_hf_name)\n \n     # Store the dataset locally as parquet\n     dataset_local_path = f\"datasets/{dataset_name}/{dataset_name}.parquet\"\n     os.makedirs(os.path.dirname(dataset_local_path), exist_ok=True)\n-    dataset.to_pandas().to_parquet(dataset_local_path)\n+    dataset[\"test\"].to_pandas().to_parquet(dataset_local_path)\n \n     # Get the instance IDs\n     if random_sampling:\n-        instances_ids = dataset['train'].shuffle(DEFAULT_SEED).select( \\\n+        instances_ids = dataset[\"test\"].shuffle(DEFAULT_SEED).select( \\\n                         range(number_of_instances))['instance_id']\n     else:\n-        instances_ids = dataset['train'].select(range(number_of_instances)) \\\n+        instances_ids = dataset[\"test\"].select(range(number_of_instances)) \\\n                                                 ['instance_id']\n \n     return instances_ids\n@@ -149,7 +150,7 @@ def setup_instance(dataset_name:str, instance_id: str):\n         raise\n \n     # Load dataset information from datasets.yml\n-    with open(\"/datasets/datasets.yml\", \"r\") as f:\n+    with open(\"datasets/datasets.yml\", \"r\") as f:\n         datasets = yaml.safe_load(f)\n \n     dataset_info = datasets.get(dataset_name)\n@@ -162,17 +163,20 @@ def setup_instance(dataset_name:str, instance_id: str):\n     if not os.path.exists(dataset_path):\n         raise ValueError(f\"Dataset file not found at {dataset_path}\")\n \n-    dataset = pd.read_parquet(dataset_path).to_dict(orient=\"records\")\n+    dataset = pd.read_parquet(dataset_path)\n \n-    # Find the instance by instance_id\n-    instance = next((item for item in dataset if item[\"instance_id\"] == instance_id), \n-                    None)\n-    if not instance:\n+    # Find the instance by instance_id \n+    instance = dataset[dataset[\"instance_id\"] == instance_id].iloc[0]\n+\n+    if instance.empty:\n         raise ValueError(f\"Instance {instance_id} not found in dataset\")\n \n     # Pull the repo at the specific commit\n     base_commit = instance[\"base_commit\"]\n-    os.system(f\"git clone {dataset_info['repo_url']} repo\")\n+    # empty the repo directory\n+    shutil.rmtree(\"repo\")\n+    # clone the repo\n+    os.system(f\"git clone https://github.com/{instance['repo']}.git repo\")\n     os.chdir(\"repo\")\n     os.system(f\"git checkout {base_commit}\")\n \n@@ -292,9 +296,9 @@ def main() -> None:\n         experiment_name, skipped_instance = run_ai()\n         if not skipped_instance:\n             append_to_inferences(inferences_path=os.path.join(\"results\", \n-                                os.path.join(\"subresults\", \"inference.jsonl\"), \\\n+                                os.path.join(\"subresults\", \"inference.jsonl\")), \\\n                                 instance_id=instance_id, \\\n-                                experiment_name=experiment_name))\n+                                experiment_name=experiment_name)\n         else:\n             skipped_instances += 1\n     print(skipped_instances)\ndiff --git a/experimentation/datasets/datasets.yml b/experimentation/datasets/datasets.yml\nindex 8bf08d0..d2f5984 100755\n--- a/experimentation/datasets/datasets.yml\n+++ b/experimentation/datasets/datasets.yml\n@@ -1,3 +1,3 @@\n swe_bench_verified: \n-  url: https://huggingface.co/datasets/princeton-nlp/SWE-bench_Verified\n+  hf_name: princeton-nlp/SWE-bench_Verified\n   local_path: datasets/swe_verified/swe_verified.parquet\n\\ No newline at end of file\ndiff --git a/src/llmops_issue_resolver/cli.py b/src/llmops_issue_resolver/cli.py\nindex 51072c6..37f821e 100644\n--- a/src/llmops_issue_resolver/cli.py\n+++ b/src/llmops_issue_resolver/cli.py\n@@ -36,7 +36,7 @@ def temporary_sys_path(path):\n #     tree\n # All .py modules need to have this line, but with the more general form of the import \n \n-with temporary_sys_path(os.path.abspath(os.join(\"../\", os.path.dirname(__file__)))):\n+with temporary_sys_path(os.path.abspath(os.join(os.path.dirname(__file__), \"../\"))):\n     from llmops_issue_resolver.agent import graph\n \n app = typer.Typer()\n", "model_name_or_path": "placeholder__placeholder__placeholder__placeholder__placeholder"}
{"instance_id": "sympy__sympy-19783", "model_patch": "diff --git a/experimentation/SWE-bench b/experimentation/SWE-bench\n--- a/experimentation/SWE-bench\n+++ b/experimentation/SWE-bench\n@@ -1 +1 @@\n-Subproject commit 5f5a7df799663adba4b191eca3d675faf3621fe2\n+Subproject commit 5f5a7df799663adba4b191eca3d675faf3621fe2-dirty\ndiff --git a/experimentation/code/scripts/generate_inferences.py b/experimentation/code/scripts/generate_inferences.py\nindex 0b9a2bb..27c731e 100755\n--- a/experimentation/code/scripts/generate_inferences.py\n+++ b/experimentation/code/scripts/generate_inferences.py\n@@ -3,13 +3,14 @@\n import argparse\n import json\n import os\n+import shutil\n import sys\n from contextlib import contextmanager\n from typing import List\n \n-import datasets as ds\n import pandas as pd\n import yaml\n+from datasets import load_dataset\n from pydantic import ValidationError\n \n \n@@ -89,23 +90,23 @@ def get_instances_ids(dataset_pointer_path: str, dataset_name: str, \\\n     with open(dataset_pointer_path, \"r\") as f:\n         datasets = yaml.safe_load(f)\n \n-    dataset_hf_path = datasets[dataset_name][\"url\"]\n-    print(f\"Dataset hugging face path: {dataset_hf_path}\")\n+    dataset_hf_name= datasets[dataset_name][\"hf_name\"]\n+    print(f\"dataset_hf_name: {dataset_hf_name}\")\n \n-    # Load the dataset from the the huggin face dataset path\n-    dataset = ds.load_dataset(dataset_hf_path, name=dataset_name)\n+    # Load the dataset from the the hugging face dataset path\n+    dataset = load_dataset(dataset_hf_name)\n \n     # Store the dataset locally as parquet\n     dataset_local_path = f\"datasets/{dataset_name}/{dataset_name}.parquet\"\n     os.makedirs(os.path.dirname(dataset_local_path), exist_ok=True)\n-    dataset.to_pandas().to_parquet(dataset_local_path)\n+    dataset[\"test\"].to_pandas().to_parquet(dataset_local_path)\n \n     # Get the instance IDs\n     if random_sampling:\n-        instances_ids = dataset['train'].shuffle(DEFAULT_SEED).select( \\\n+        instances_ids = dataset[\"test\"].shuffle(DEFAULT_SEED).select( \\\n                         range(number_of_instances))['instance_id']\n     else:\n-        instances_ids = dataset['train'].select(range(number_of_instances)) \\\n+        instances_ids = dataset[\"test\"].select(range(number_of_instances)) \\\n                                                 ['instance_id']\n \n     return instances_ids\n@@ -149,7 +150,7 @@ def setup_instance(dataset_name:str, instance_id: str):\n         raise\n \n     # Load dataset information from datasets.yml\n-    with open(\"/datasets/datasets.yml\", \"r\") as f:\n+    with open(\"datasets/datasets.yml\", \"r\") as f:\n         datasets = yaml.safe_load(f)\n \n     dataset_info = datasets.get(dataset_name)\n@@ -162,17 +163,20 @@ def setup_instance(dataset_name:str, instance_id: str):\n     if not os.path.exists(dataset_path):\n         raise ValueError(f\"Dataset file not found at {dataset_path}\")\n \n-    dataset = pd.read_parquet(dataset_path).to_dict(orient=\"records\")\n+    dataset = pd.read_parquet(dataset_path)\n \n-    # Find the instance by instance_id\n-    instance = next((item for item in dataset if item[\"instance_id\"] == instance_id), \n-                    None)\n-    if not instance:\n+    # Find the instance by instance_id \n+    instance = dataset[dataset[\"instance_id\"] == instance_id].iloc[0]\n+\n+    if instance.empty:\n         raise ValueError(f\"Instance {instance_id} not found in dataset\")\n \n     # Pull the repo at the specific commit\n     base_commit = instance[\"base_commit\"]\n-    os.system(f\"git clone {dataset_info['repo_url']} repo\")\n+    # empty the repo directory\n+    shutil.rmtree(\"repo\")\n+    # clone the repo\n+    os.system(f\"git clone https://github.com/{instance['repo']}.git repo\")\n     os.chdir(\"repo\")\n     os.system(f\"git checkout {base_commit}\")\n \n@@ -292,9 +296,9 @@ def main() -> None:\n         experiment_name, skipped_instance = run_ai()\n         if not skipped_instance:\n             append_to_inferences(inferences_path=os.path.join(\"results\", \n-                                os.path.join(\"subresults\", \"inference.jsonl\"), \\\n+                                os.path.join(\"subresults\", \"inference.jsonl\")), \\\n                                 instance_id=instance_id, \\\n-                                experiment_name=experiment_name))\n+                                experiment_name=experiment_name)\n         else:\n             skipped_instances += 1\n     print(skipped_instances)\ndiff --git a/experimentation/datasets/datasets.yml b/experimentation/datasets/datasets.yml\nindex 8bf08d0..d2f5984 100755\n--- a/experimentation/datasets/datasets.yml\n+++ b/experimentation/datasets/datasets.yml\n@@ -1,3 +1,3 @@\n swe_bench_verified: \n-  url: https://huggingface.co/datasets/princeton-nlp/SWE-bench_Verified\n+  hf_name: princeton-nlp/SWE-bench_Verified\n   local_path: datasets/swe_verified/swe_verified.parquet\n\\ No newline at end of file\ndiff --git a/src/llmops_issue_resolver/cli.py b/src/llmops_issue_resolver/cli.py\nindex 51072c6..37f821e 100644\n--- a/src/llmops_issue_resolver/cli.py\n+++ b/src/llmops_issue_resolver/cli.py\n@@ -36,7 +36,7 @@ def temporary_sys_path(path):\n #     tree\n # All .py modules need to have this line, but with the more general form of the import \n \n-with temporary_sys_path(os.path.abspath(os.join(\"../\", os.path.dirname(__file__)))):\n+with temporary_sys_path(os.path.abspath(os.join(os.path.dirname(__file__), \"../\"))):\n     from llmops_issue_resolver.agent import graph\n \n app = typer.Typer()\n", "model_name_or_path": "placeholder__placeholder__placeholder__placeholder__placeholder"}
{"instance_id": "django__django-11333", "model_patch": "diff --git a/experimentation/SWE-bench b/experimentation/SWE-bench\n--- a/experimentation/SWE-bench\n+++ b/experimentation/SWE-bench\n@@ -1 +1 @@\n-Subproject commit 5f5a7df799663adba4b191eca3d675faf3621fe2\n+Subproject commit 5f5a7df799663adba4b191eca3d675faf3621fe2-dirty\ndiff --git a/experimentation/code/scripts/generate_inferences.py b/experimentation/code/scripts/generate_inferences.py\nindex 0b9a2bb..27c731e 100755\n--- a/experimentation/code/scripts/generate_inferences.py\n+++ b/experimentation/code/scripts/generate_inferences.py\n@@ -3,13 +3,14 @@\n import argparse\n import json\n import os\n+import shutil\n import sys\n from contextlib import contextmanager\n from typing import List\n \n-import datasets as ds\n import pandas as pd\n import yaml\n+from datasets import load_dataset\n from pydantic import ValidationError\n \n \n@@ -89,23 +90,23 @@ def get_instances_ids(dataset_pointer_path: str, dataset_name: str, \\\n     with open(dataset_pointer_path, \"r\") as f:\n         datasets = yaml.safe_load(f)\n \n-    dataset_hf_path = datasets[dataset_name][\"url\"]\n-    print(f\"Dataset hugging face path: {dataset_hf_path}\")\n+    dataset_hf_name= datasets[dataset_name][\"hf_name\"]\n+    print(f\"dataset_hf_name: {dataset_hf_name}\")\n \n-    # Load the dataset from the the huggin face dataset path\n-    dataset = ds.load_dataset(dataset_hf_path, name=dataset_name)\n+    # Load the dataset from the the hugging face dataset path\n+    dataset = load_dataset(dataset_hf_name)\n \n     # Store the dataset locally as parquet\n     dataset_local_path = f\"datasets/{dataset_name}/{dataset_name}.parquet\"\n     os.makedirs(os.path.dirname(dataset_local_path), exist_ok=True)\n-    dataset.to_pandas().to_parquet(dataset_local_path)\n+    dataset[\"test\"].to_pandas().to_parquet(dataset_local_path)\n \n     # Get the instance IDs\n     if random_sampling:\n-        instances_ids = dataset['train'].shuffle(DEFAULT_SEED).select( \\\n+        instances_ids = dataset[\"test\"].shuffle(DEFAULT_SEED).select( \\\n                         range(number_of_instances))['instance_id']\n     else:\n-        instances_ids = dataset['train'].select(range(number_of_instances)) \\\n+        instances_ids = dataset[\"test\"].select(range(number_of_instances)) \\\n                                                 ['instance_id']\n \n     return instances_ids\n@@ -149,7 +150,7 @@ def setup_instance(dataset_name:str, instance_id: str):\n         raise\n \n     # Load dataset information from datasets.yml\n-    with open(\"/datasets/datasets.yml\", \"r\") as f:\n+    with open(\"datasets/datasets.yml\", \"r\") as f:\n         datasets = yaml.safe_load(f)\n \n     dataset_info = datasets.get(dataset_name)\n@@ -162,17 +163,20 @@ def setup_instance(dataset_name:str, instance_id: str):\n     if not os.path.exists(dataset_path):\n         raise ValueError(f\"Dataset file not found at {dataset_path}\")\n \n-    dataset = pd.read_parquet(dataset_path).to_dict(orient=\"records\")\n+    dataset = pd.read_parquet(dataset_path)\n \n-    # Find the instance by instance_id\n-    instance = next((item for item in dataset if item[\"instance_id\"] == instance_id), \n-                    None)\n-    if not instance:\n+    # Find the instance by instance_id \n+    instance = dataset[dataset[\"instance_id\"] == instance_id].iloc[0]\n+\n+    if instance.empty:\n         raise ValueError(f\"Instance {instance_id} not found in dataset\")\n \n     # Pull the repo at the specific commit\n     base_commit = instance[\"base_commit\"]\n-    os.system(f\"git clone {dataset_info['repo_url']} repo\")\n+    # empty the repo directory\n+    shutil.rmtree(\"repo\")\n+    # clone the repo\n+    os.system(f\"git clone https://github.com/{instance['repo']}.git repo\")\n     os.chdir(\"repo\")\n     os.system(f\"git checkout {base_commit}\")\n \n@@ -292,9 +296,9 @@ def main() -> None:\n         experiment_name, skipped_instance = run_ai()\n         if not skipped_instance:\n             append_to_inferences(inferences_path=os.path.join(\"results\", \n-                                os.path.join(\"subresults\", \"inference.jsonl\"), \\\n+                                os.path.join(\"subresults\", \"inference.jsonl\")), \\\n                                 instance_id=instance_id, \\\n-                                experiment_name=experiment_name))\n+                                experiment_name=experiment_name)\n         else:\n             skipped_instances += 1\n     print(skipped_instances)\ndiff --git a/experimentation/datasets/datasets.yml b/experimentation/datasets/datasets.yml\nindex 8bf08d0..d2f5984 100755\n--- a/experimentation/datasets/datasets.yml\n+++ b/experimentation/datasets/datasets.yml\n@@ -1,3 +1,3 @@\n swe_bench_verified: \n-  url: https://huggingface.co/datasets/princeton-nlp/SWE-bench_Verified\n+  hf_name: princeton-nlp/SWE-bench_Verified\n   local_path: datasets/swe_verified/swe_verified.parquet\n\\ No newline at end of file\ndiff --git a/src/llmops_issue_resolver/cli.py b/src/llmops_issue_resolver/cli.py\nindex 51072c6..37f821e 100644\n--- a/src/llmops_issue_resolver/cli.py\n+++ b/src/llmops_issue_resolver/cli.py\n@@ -36,7 +36,7 @@ def temporary_sys_path(path):\n #     tree\n # All .py modules need to have this line, but with the more general form of the import \n \n-with temporary_sys_path(os.path.abspath(os.join(\"../\", os.path.dirname(__file__)))):\n+with temporary_sys_path(os.path.abspath(os.join(os.path.dirname(__file__), \"../\"))):\n     from llmops_issue_resolver.agent import graph\n \n app = typer.Typer()\n", "model_name_or_path": "placeholder__placeholder__placeholder__placeholder__placeholder"}
{"instance_id": "django__django-11211", "model_patch": "diff --git a/experimentation/SWE-bench b/experimentation/SWE-bench\n--- a/experimentation/SWE-bench\n+++ b/experimentation/SWE-bench\n@@ -1 +1 @@\n-Subproject commit 5f5a7df799663adba4b191eca3d675faf3621fe2\n+Subproject commit 5f5a7df799663adba4b191eca3d675faf3621fe2-dirty\ndiff --git a/experimentation/code/scripts/generate_inferences.py b/experimentation/code/scripts/generate_inferences.py\nindex 0b9a2bb..27c731e 100755\n--- a/experimentation/code/scripts/generate_inferences.py\n+++ b/experimentation/code/scripts/generate_inferences.py\n@@ -3,13 +3,14 @@\n import argparse\n import json\n import os\n+import shutil\n import sys\n from contextlib import contextmanager\n from typing import List\n \n-import datasets as ds\n import pandas as pd\n import yaml\n+from datasets import load_dataset\n from pydantic import ValidationError\n \n \n@@ -89,23 +90,23 @@ def get_instances_ids(dataset_pointer_path: str, dataset_name: str, \\\n     with open(dataset_pointer_path, \"r\") as f:\n         datasets = yaml.safe_load(f)\n \n-    dataset_hf_path = datasets[dataset_name][\"url\"]\n-    print(f\"Dataset hugging face path: {dataset_hf_path}\")\n+    dataset_hf_name= datasets[dataset_name][\"hf_name\"]\n+    print(f\"dataset_hf_name: {dataset_hf_name}\")\n \n-    # Load the dataset from the the huggin face dataset path\n-    dataset = ds.load_dataset(dataset_hf_path, name=dataset_name)\n+    # Load the dataset from the the hugging face dataset path\n+    dataset = load_dataset(dataset_hf_name)\n \n     # Store the dataset locally as parquet\n     dataset_local_path = f\"datasets/{dataset_name}/{dataset_name}.parquet\"\n     os.makedirs(os.path.dirname(dataset_local_path), exist_ok=True)\n-    dataset.to_pandas().to_parquet(dataset_local_path)\n+    dataset[\"test\"].to_pandas().to_parquet(dataset_local_path)\n \n     # Get the instance IDs\n     if random_sampling:\n-        instances_ids = dataset['train'].shuffle(DEFAULT_SEED).select( \\\n+        instances_ids = dataset[\"test\"].shuffle(DEFAULT_SEED).select( \\\n                         range(number_of_instances))['instance_id']\n     else:\n-        instances_ids = dataset['train'].select(range(number_of_instances)) \\\n+        instances_ids = dataset[\"test\"].select(range(number_of_instances)) \\\n                                                 ['instance_id']\n \n     return instances_ids\n@@ -149,7 +150,7 @@ def setup_instance(dataset_name:str, instance_id: str):\n         raise\n \n     # Load dataset information from datasets.yml\n-    with open(\"/datasets/datasets.yml\", \"r\") as f:\n+    with open(\"datasets/datasets.yml\", \"r\") as f:\n         datasets = yaml.safe_load(f)\n \n     dataset_info = datasets.get(dataset_name)\n@@ -162,17 +163,20 @@ def setup_instance(dataset_name:str, instance_id: str):\n     if not os.path.exists(dataset_path):\n         raise ValueError(f\"Dataset file not found at {dataset_path}\")\n \n-    dataset = pd.read_parquet(dataset_path).to_dict(orient=\"records\")\n+    dataset = pd.read_parquet(dataset_path)\n \n-    # Find the instance by instance_id\n-    instance = next((item for item in dataset if item[\"instance_id\"] == instance_id), \n-                    None)\n-    if not instance:\n+    # Find the instance by instance_id \n+    instance = dataset[dataset[\"instance_id\"] == instance_id].iloc[0]\n+\n+    if instance.empty:\n         raise ValueError(f\"Instance {instance_id} not found in dataset\")\n \n     # Pull the repo at the specific commit\n     base_commit = instance[\"base_commit\"]\n-    os.system(f\"git clone {dataset_info['repo_url']} repo\")\n+    # empty the repo directory\n+    shutil.rmtree(\"repo\")\n+    # clone the repo\n+    os.system(f\"git clone https://github.com/{instance['repo']}.git repo\")\n     os.chdir(\"repo\")\n     os.system(f\"git checkout {base_commit}\")\n \n@@ -292,9 +296,9 @@ def main() -> None:\n         experiment_name, skipped_instance = run_ai()\n         if not skipped_instance:\n             append_to_inferences(inferences_path=os.path.join(\"results\", \n-                                os.path.join(\"subresults\", \"inference.jsonl\"), \\\n+                                os.path.join(\"subresults\", \"inference.jsonl\")), \\\n                                 instance_id=instance_id, \\\n-                                experiment_name=experiment_name))\n+                                experiment_name=experiment_name)\n         else:\n             skipped_instances += 1\n     print(skipped_instances)\ndiff --git a/experimentation/datasets/datasets.yml b/experimentation/datasets/datasets.yml\nindex 8bf08d0..d2f5984 100755\n--- a/experimentation/datasets/datasets.yml\n+++ b/experimentation/datasets/datasets.yml\n@@ -1,3 +1,3 @@\n swe_bench_verified: \n-  url: https://huggingface.co/datasets/princeton-nlp/SWE-bench_Verified\n+  hf_name: princeton-nlp/SWE-bench_Verified\n   local_path: datasets/swe_verified/swe_verified.parquet\n\\ No newline at end of file\ndiff --git a/src/llmops_issue_resolver/cli.py b/src/llmops_issue_resolver/cli.py\nindex 51072c6..37f821e 100644\n--- a/src/llmops_issue_resolver/cli.py\n+++ b/src/llmops_issue_resolver/cli.py\n@@ -36,7 +36,7 @@ def temporary_sys_path(path):\n #     tree\n # All .py modules need to have this line, but with the more general form of the import \n \n-with temporary_sys_path(os.path.abspath(os.join(\"../\", os.path.dirname(__file__)))):\n+with temporary_sys_path(os.path.abspath(os.join(os.path.dirname(__file__), \"../\"))):\n     from llmops_issue_resolver.agent import graph\n \n app = typer.Typer()\n", "model_name_or_path": "placeholder__placeholder__placeholder__placeholder__placeholder"}
{"instance_id": "sympy__sympy-19954", "model_patch": "diff --git a/experimentation/SWE-bench b/experimentation/SWE-bench\n--- a/experimentation/SWE-bench\n+++ b/experimentation/SWE-bench\n@@ -1 +1 @@\n-Subproject commit 5f5a7df799663adba4b191eca3d675faf3621fe2\n+Subproject commit 5f5a7df799663adba4b191eca3d675faf3621fe2-dirty\ndiff --git a/experimentation/code/scripts/generate_inferences.py b/experimentation/code/scripts/generate_inferences.py\nindex 0b9a2bb..e9b3a4e 100755\n--- a/experimentation/code/scripts/generate_inferences.py\n+++ b/experimentation/code/scripts/generate_inferences.py\n@@ -3,13 +3,14 @@\n import argparse\n import json\n import os\n+import shutil\n import sys\n from contextlib import contextmanager\n from typing import List\n \n-import datasets as ds\n import pandas as pd\n import yaml\n+from datasets import load_dataset\n from pydantic import ValidationError\n \n \n@@ -89,23 +90,23 @@ def get_instances_ids(dataset_pointer_path: str, dataset_name: str, \\\n     with open(dataset_pointer_path, \"r\") as f:\n         datasets = yaml.safe_load(f)\n \n-    dataset_hf_path = datasets[dataset_name][\"url\"]\n-    print(f\"Dataset hugging face path: {dataset_hf_path}\")\n+    dataset_hf_name= datasets[dataset_name][\"hf_name\"]\n+    print(f\"dataset_hf_name: {dataset_hf_name}\")\n \n-    # Load the dataset from the the huggin face dataset path\n-    dataset = ds.load_dataset(dataset_hf_path, name=dataset_name)\n+    # Load the dataset from the the hugging face dataset path\n+    dataset = load_dataset(dataset_hf_name)\n \n     # Store the dataset locally as parquet\n     dataset_local_path = f\"datasets/{dataset_name}/{dataset_name}.parquet\"\n     os.makedirs(os.path.dirname(dataset_local_path), exist_ok=True)\n-    dataset.to_pandas().to_parquet(dataset_local_path)\n+    dataset[\"test\"].to_pandas().to_parquet(dataset_local_path)\n \n     # Get the instance IDs\n     if random_sampling:\n-        instances_ids = dataset['train'].shuffle(DEFAULT_SEED).select( \\\n+        instances_ids = dataset[\"test\"].shuffle(DEFAULT_SEED).select( \\\n                         range(number_of_instances))['instance_id']\n     else:\n-        instances_ids = dataset['train'].select(range(number_of_instances)) \\\n+        instances_ids = dataset[\"test\"].select(range(number_of_instances)) \\\n                                                 ['instance_id']\n \n     return instances_ids\n@@ -149,7 +150,7 @@ def setup_instance(dataset_name:str, instance_id: str):\n         raise\n \n     # Load dataset information from datasets.yml\n-    with open(\"/datasets/datasets.yml\", \"r\") as f:\n+    with open(\"datasets/datasets.yml\", \"r\") as f:\n         datasets = yaml.safe_load(f)\n \n     dataset_info = datasets.get(dataset_name)\n@@ -162,17 +163,20 @@ def setup_instance(dataset_name:str, instance_id: str):\n     if not os.path.exists(dataset_path):\n         raise ValueError(f\"Dataset file not found at {dataset_path}\")\n \n-    dataset = pd.read_parquet(dataset_path).to_dict(orient=\"records\")\n+    dataset = pd.read_parquet(dataset_path)\n \n-    # Find the instance by instance_id\n-    instance = next((item for item in dataset if item[\"instance_id\"] == instance_id), \n-                    None)\n-    if not instance:\n+    # Find the instance by instance_id \n+    instance = dataset[dataset[\"instance_id\"] == instance_id].iloc[0]\n+\n+    if instance.empty:\n         raise ValueError(f\"Instance {instance_id} not found in dataset\")\n \n     # Pull the repo at the specific commit\n     base_commit = instance[\"base_commit\"]\n-    os.system(f\"git clone {dataset_info['repo_url']} repo\")\n+    # empty the repo directory\n+    shutil.rmtree(\"repo\")\n+    # clone the repo\n+    os.system(f\"git clone https://github.com/{instance['repo']}.git repo\")\n     os.chdir(\"repo\")\n     os.system(f\"git checkout {base_commit}\")\n \n@@ -289,12 +293,16 @@ def main() -> None:\n     skipped_instances = 0\n     for instance_id in instances_ids:\n         setup_instance(dataset_name=dataset_name, instance_id=instance_id)\n+        # change current working directory to the repo directory\n+        os.chdir(\"repo\")\n         experiment_name, skipped_instance = run_ai()\n+        # change current working directory to the parent directory\n+        os.chdir(\"..\")\n         if not skipped_instance:\n             append_to_inferences(inferences_path=os.path.join(\"results\", \n-                                os.path.join(\"subresults\", \"inference.jsonl\"), \\\n+                                os.path.join(\"subresults\", \"inference.jsonl\")), \\\n                                 instance_id=instance_id, \\\n-                                experiment_name=experiment_name))\n+                                experiment_name=experiment_name)\n         else:\n             skipped_instances += 1\n     print(skipped_instances)\ndiff --git a/experimentation/datasets/datasets.yml b/experimentation/datasets/datasets.yml\nindex 8bf08d0..d2f5984 100755\n--- a/experimentation/datasets/datasets.yml\n+++ b/experimentation/datasets/datasets.yml\n@@ -1,3 +1,3 @@\n swe_bench_verified: \n-  url: https://huggingface.co/datasets/princeton-nlp/SWE-bench_Verified\n+  hf_name: princeton-nlp/SWE-bench_Verified\n   local_path: datasets/swe_verified/swe_verified.parquet\n\\ No newline at end of file\ndiff --git a/src/llmops_issue_resolver/cli.py b/src/llmops_issue_resolver/cli.py\nindex 51072c6..37f821e 100644\n--- a/src/llmops_issue_resolver/cli.py\n+++ b/src/llmops_issue_resolver/cli.py\n@@ -36,7 +36,7 @@ def temporary_sys_path(path):\n #     tree\n # All .py modules need to have this line, but with the more general form of the import \n \n-with temporary_sys_path(os.path.abspath(os.join(\"../\", os.path.dirname(__file__)))):\n+with temporary_sys_path(os.path.abspath(os.join(os.path.dirname(__file__), \"../\"))):\n     from llmops_issue_resolver.agent import graph\n \n app = typer.Typer()\n", "model_name_or_path": "placeholder__placeholder__placeholder__placeholder__placeholder"}
{"instance_id": "django__django-13821", "model_patch": "diff --git a/experimentation/SWE-bench b/experimentation/SWE-bench\n--- a/experimentation/SWE-bench\n+++ b/experimentation/SWE-bench\n@@ -1 +1 @@\n-Subproject commit 5f5a7df799663adba4b191eca3d675faf3621fe2\n+Subproject commit 5f5a7df799663adba4b191eca3d675faf3621fe2-dirty\ndiff --git a/experimentation/code/scripts/generate_inferences.py b/experimentation/code/scripts/generate_inferences.py\nindex 0b9a2bb..e9b3a4e 100755\n--- a/experimentation/code/scripts/generate_inferences.py\n+++ b/experimentation/code/scripts/generate_inferences.py\n@@ -3,13 +3,14 @@\n import argparse\n import json\n import os\n+import shutil\n import sys\n from contextlib import contextmanager\n from typing import List\n \n-import datasets as ds\n import pandas as pd\n import yaml\n+from datasets import load_dataset\n from pydantic import ValidationError\n \n \n@@ -89,23 +90,23 @@ def get_instances_ids(dataset_pointer_path: str, dataset_name: str, \\\n     with open(dataset_pointer_path, \"r\") as f:\n         datasets = yaml.safe_load(f)\n \n-    dataset_hf_path = datasets[dataset_name][\"url\"]\n-    print(f\"Dataset hugging face path: {dataset_hf_path}\")\n+    dataset_hf_name= datasets[dataset_name][\"hf_name\"]\n+    print(f\"dataset_hf_name: {dataset_hf_name}\")\n \n-    # Load the dataset from the the huggin face dataset path\n-    dataset = ds.load_dataset(dataset_hf_path, name=dataset_name)\n+    # Load the dataset from the the hugging face dataset path\n+    dataset = load_dataset(dataset_hf_name)\n \n     # Store the dataset locally as parquet\n     dataset_local_path = f\"datasets/{dataset_name}/{dataset_name}.parquet\"\n     os.makedirs(os.path.dirname(dataset_local_path), exist_ok=True)\n-    dataset.to_pandas().to_parquet(dataset_local_path)\n+    dataset[\"test\"].to_pandas().to_parquet(dataset_local_path)\n \n     # Get the instance IDs\n     if random_sampling:\n-        instances_ids = dataset['train'].shuffle(DEFAULT_SEED).select( \\\n+        instances_ids = dataset[\"test\"].shuffle(DEFAULT_SEED).select( \\\n                         range(number_of_instances))['instance_id']\n     else:\n-        instances_ids = dataset['train'].select(range(number_of_instances)) \\\n+        instances_ids = dataset[\"test\"].select(range(number_of_instances)) \\\n                                                 ['instance_id']\n \n     return instances_ids\n@@ -149,7 +150,7 @@ def setup_instance(dataset_name:str, instance_id: str):\n         raise\n \n     # Load dataset information from datasets.yml\n-    with open(\"/datasets/datasets.yml\", \"r\") as f:\n+    with open(\"datasets/datasets.yml\", \"r\") as f:\n         datasets = yaml.safe_load(f)\n \n     dataset_info = datasets.get(dataset_name)\n@@ -162,17 +163,20 @@ def setup_instance(dataset_name:str, instance_id: str):\n     if not os.path.exists(dataset_path):\n         raise ValueError(f\"Dataset file not found at {dataset_path}\")\n \n-    dataset = pd.read_parquet(dataset_path).to_dict(orient=\"records\")\n+    dataset = pd.read_parquet(dataset_path)\n \n-    # Find the instance by instance_id\n-    instance = next((item for item in dataset if item[\"instance_id\"] == instance_id), \n-                    None)\n-    if not instance:\n+    # Find the instance by instance_id \n+    instance = dataset[dataset[\"instance_id\"] == instance_id].iloc[0]\n+\n+    if instance.empty:\n         raise ValueError(f\"Instance {instance_id} not found in dataset\")\n \n     # Pull the repo at the specific commit\n     base_commit = instance[\"base_commit\"]\n-    os.system(f\"git clone {dataset_info['repo_url']} repo\")\n+    # empty the repo directory\n+    shutil.rmtree(\"repo\")\n+    # clone the repo\n+    os.system(f\"git clone https://github.com/{instance['repo']}.git repo\")\n     os.chdir(\"repo\")\n     os.system(f\"git checkout {base_commit}\")\n \n@@ -289,12 +293,16 @@ def main() -> None:\n     skipped_instances = 0\n     for instance_id in instances_ids:\n         setup_instance(dataset_name=dataset_name, instance_id=instance_id)\n+        # change current working directory to the repo directory\n+        os.chdir(\"repo\")\n         experiment_name, skipped_instance = run_ai()\n+        # change current working directory to the parent directory\n+        os.chdir(\"..\")\n         if not skipped_instance:\n             append_to_inferences(inferences_path=os.path.join(\"results\", \n-                                os.path.join(\"subresults\", \"inference.jsonl\"), \\\n+                                os.path.join(\"subresults\", \"inference.jsonl\")), \\\n                                 instance_id=instance_id, \\\n-                                experiment_name=experiment_name))\n+                                experiment_name=experiment_name)\n         else:\n             skipped_instances += 1\n     print(skipped_instances)\ndiff --git a/experimentation/datasets/datasets.yml b/experimentation/datasets/datasets.yml\nindex 8bf08d0..d2f5984 100755\n--- a/experimentation/datasets/datasets.yml\n+++ b/experimentation/datasets/datasets.yml\n@@ -1,3 +1,3 @@\n swe_bench_verified: \n-  url: https://huggingface.co/datasets/princeton-nlp/SWE-bench_Verified\n+  hf_name: princeton-nlp/SWE-bench_Verified\n   local_path: datasets/swe_verified/swe_verified.parquet\n\\ No newline at end of file\ndiff --git a/src/llmops_issue_resolver/cli.py b/src/llmops_issue_resolver/cli.py\nindex 51072c6..37f821e 100644\n--- a/src/llmops_issue_resolver/cli.py\n+++ b/src/llmops_issue_resolver/cli.py\n@@ -36,7 +36,7 @@ def temporary_sys_path(path):\n #     tree\n # All .py modules need to have this line, but with the more general form of the import \n \n-with temporary_sys_path(os.path.abspath(os.join(\"../\", os.path.dirname(__file__)))):\n+with temporary_sys_path(os.path.abspath(os.join(os.path.dirname(__file__), \"../\"))):\n     from llmops_issue_resolver.agent import graph\n \n app = typer.Typer()\n", "model_name_or_path": "placeholder__placeholder__placeholder__placeholder__placeholder"}
{"instance_id": "sympy__sympy-19783", "model_patch": "diff --git a/experimentation/SWE-bench b/experimentation/SWE-bench\n--- a/experimentation/SWE-bench\n+++ b/experimentation/SWE-bench\n@@ -1 +1 @@\n-Subproject commit 5f5a7df799663adba4b191eca3d675faf3621fe2\n+Subproject commit 5f5a7df799663adba4b191eca3d675faf3621fe2-dirty\ndiff --git a/experimentation/code/scripts/generate_inferences.py b/experimentation/code/scripts/generate_inferences.py\nindex 0b9a2bb..e9b3a4e 100755\n--- a/experimentation/code/scripts/generate_inferences.py\n+++ b/experimentation/code/scripts/generate_inferences.py\n@@ -3,13 +3,14 @@\n import argparse\n import json\n import os\n+import shutil\n import sys\n from contextlib import contextmanager\n from typing import List\n \n-import datasets as ds\n import pandas as pd\n import yaml\n+from datasets import load_dataset\n from pydantic import ValidationError\n \n \n@@ -89,23 +90,23 @@ def get_instances_ids(dataset_pointer_path: str, dataset_name: str, \\\n     with open(dataset_pointer_path, \"r\") as f:\n         datasets = yaml.safe_load(f)\n \n-    dataset_hf_path = datasets[dataset_name][\"url\"]\n-    print(f\"Dataset hugging face path: {dataset_hf_path}\")\n+    dataset_hf_name= datasets[dataset_name][\"hf_name\"]\n+    print(f\"dataset_hf_name: {dataset_hf_name}\")\n \n-    # Load the dataset from the the huggin face dataset path\n-    dataset = ds.load_dataset(dataset_hf_path, name=dataset_name)\n+    # Load the dataset from the the hugging face dataset path\n+    dataset = load_dataset(dataset_hf_name)\n \n     # Store the dataset locally as parquet\n     dataset_local_path = f\"datasets/{dataset_name}/{dataset_name}.parquet\"\n     os.makedirs(os.path.dirname(dataset_local_path), exist_ok=True)\n-    dataset.to_pandas().to_parquet(dataset_local_path)\n+    dataset[\"test\"].to_pandas().to_parquet(dataset_local_path)\n \n     # Get the instance IDs\n     if random_sampling:\n-        instances_ids = dataset['train'].shuffle(DEFAULT_SEED).select( \\\n+        instances_ids = dataset[\"test\"].shuffle(DEFAULT_SEED).select( \\\n                         range(number_of_instances))['instance_id']\n     else:\n-        instances_ids = dataset['train'].select(range(number_of_instances)) \\\n+        instances_ids = dataset[\"test\"].select(range(number_of_instances)) \\\n                                                 ['instance_id']\n \n     return instances_ids\n@@ -149,7 +150,7 @@ def setup_instance(dataset_name:str, instance_id: str):\n         raise\n \n     # Load dataset information from datasets.yml\n-    with open(\"/datasets/datasets.yml\", \"r\") as f:\n+    with open(\"datasets/datasets.yml\", \"r\") as f:\n         datasets = yaml.safe_load(f)\n \n     dataset_info = datasets.get(dataset_name)\n@@ -162,17 +163,20 @@ def setup_instance(dataset_name:str, instance_id: str):\n     if not os.path.exists(dataset_path):\n         raise ValueError(f\"Dataset file not found at {dataset_path}\")\n \n-    dataset = pd.read_parquet(dataset_path).to_dict(orient=\"records\")\n+    dataset = pd.read_parquet(dataset_path)\n \n-    # Find the instance by instance_id\n-    instance = next((item for item in dataset if item[\"instance_id\"] == instance_id), \n-                    None)\n-    if not instance:\n+    # Find the instance by instance_id \n+    instance = dataset[dataset[\"instance_id\"] == instance_id].iloc[0]\n+\n+    if instance.empty:\n         raise ValueError(f\"Instance {instance_id} not found in dataset\")\n \n     # Pull the repo at the specific commit\n     base_commit = instance[\"base_commit\"]\n-    os.system(f\"git clone {dataset_info['repo_url']} repo\")\n+    # empty the repo directory\n+    shutil.rmtree(\"repo\")\n+    # clone the repo\n+    os.system(f\"git clone https://github.com/{instance['repo']}.git repo\")\n     os.chdir(\"repo\")\n     os.system(f\"git checkout {base_commit}\")\n \n@@ -289,12 +293,16 @@ def main() -> None:\n     skipped_instances = 0\n     for instance_id in instances_ids:\n         setup_instance(dataset_name=dataset_name, instance_id=instance_id)\n+        # change current working directory to the repo directory\n+        os.chdir(\"repo\")\n         experiment_name, skipped_instance = run_ai()\n+        # change current working directory to the parent directory\n+        os.chdir(\"..\")\n         if not skipped_instance:\n             append_to_inferences(inferences_path=os.path.join(\"results\", \n-                                os.path.join(\"subresults\", \"inference.jsonl\"), \\\n+                                os.path.join(\"subresults\", \"inference.jsonl\")), \\\n                                 instance_id=instance_id, \\\n-                                experiment_name=experiment_name))\n+                                experiment_name=experiment_name)\n         else:\n             skipped_instances += 1\n     print(skipped_instances)\ndiff --git a/experimentation/datasets/datasets.yml b/experimentation/datasets/datasets.yml\nindex 8bf08d0..d2f5984 100755\n--- a/experimentation/datasets/datasets.yml\n+++ b/experimentation/datasets/datasets.yml\n@@ -1,3 +1,3 @@\n swe_bench_verified: \n-  url: https://huggingface.co/datasets/princeton-nlp/SWE-bench_Verified\n+  hf_name: princeton-nlp/SWE-bench_Verified\n   local_path: datasets/swe_verified/swe_verified.parquet\n\\ No newline at end of file\ndiff --git a/src/llmops_issue_resolver/cli.py b/src/llmops_issue_resolver/cli.py\nindex 51072c6..37f821e 100644\n--- a/src/llmops_issue_resolver/cli.py\n+++ b/src/llmops_issue_resolver/cli.py\n@@ -36,7 +36,7 @@ def temporary_sys_path(path):\n #     tree\n # All .py modules need to have this line, but with the more general form of the import \n \n-with temporary_sys_path(os.path.abspath(os.join(\"../\", os.path.dirname(__file__)))):\n+with temporary_sys_path(os.path.abspath(os.join(os.path.dirname(__file__), \"../\"))):\n     from llmops_issue_resolver.agent import graph\n \n app = typer.Typer()\n", "model_name_or_path": "placeholder__placeholder__placeholder__placeholder__placeholder"}
{"instance_id": "django__django-11333", "model_patch": "diff --git a/experimentation/SWE-bench b/experimentation/SWE-bench\n--- a/experimentation/SWE-bench\n+++ b/experimentation/SWE-bench\n@@ -1 +1 @@\n-Subproject commit 5f5a7df799663adba4b191eca3d675faf3621fe2\n+Subproject commit 5f5a7df799663adba4b191eca3d675faf3621fe2-dirty\ndiff --git a/experimentation/code/scripts/generate_inferences.py b/experimentation/code/scripts/generate_inferences.py\nindex 0b9a2bb..e9b3a4e 100755\n--- a/experimentation/code/scripts/generate_inferences.py\n+++ b/experimentation/code/scripts/generate_inferences.py\n@@ -3,13 +3,14 @@\n import argparse\n import json\n import os\n+import shutil\n import sys\n from contextlib import contextmanager\n from typing import List\n \n-import datasets as ds\n import pandas as pd\n import yaml\n+from datasets import load_dataset\n from pydantic import ValidationError\n \n \n@@ -89,23 +90,23 @@ def get_instances_ids(dataset_pointer_path: str, dataset_name: str, \\\n     with open(dataset_pointer_path, \"r\") as f:\n         datasets = yaml.safe_load(f)\n \n-    dataset_hf_path = datasets[dataset_name][\"url\"]\n-    print(f\"Dataset hugging face path: {dataset_hf_path}\")\n+    dataset_hf_name= datasets[dataset_name][\"hf_name\"]\n+    print(f\"dataset_hf_name: {dataset_hf_name}\")\n \n-    # Load the dataset from the the huggin face dataset path\n-    dataset = ds.load_dataset(dataset_hf_path, name=dataset_name)\n+    # Load the dataset from the the hugging face dataset path\n+    dataset = load_dataset(dataset_hf_name)\n \n     # Store the dataset locally as parquet\n     dataset_local_path = f\"datasets/{dataset_name}/{dataset_name}.parquet\"\n     os.makedirs(os.path.dirname(dataset_local_path), exist_ok=True)\n-    dataset.to_pandas().to_parquet(dataset_local_path)\n+    dataset[\"test\"].to_pandas().to_parquet(dataset_local_path)\n \n     # Get the instance IDs\n     if random_sampling:\n-        instances_ids = dataset['train'].shuffle(DEFAULT_SEED).select( \\\n+        instances_ids = dataset[\"test\"].shuffle(DEFAULT_SEED).select( \\\n                         range(number_of_instances))['instance_id']\n     else:\n-        instances_ids = dataset['train'].select(range(number_of_instances)) \\\n+        instances_ids = dataset[\"test\"].select(range(number_of_instances)) \\\n                                                 ['instance_id']\n \n     return instances_ids\n@@ -149,7 +150,7 @@ def setup_instance(dataset_name:str, instance_id: str):\n         raise\n \n     # Load dataset information from datasets.yml\n-    with open(\"/datasets/datasets.yml\", \"r\") as f:\n+    with open(\"datasets/datasets.yml\", \"r\") as f:\n         datasets = yaml.safe_load(f)\n \n     dataset_info = datasets.get(dataset_name)\n@@ -162,17 +163,20 @@ def setup_instance(dataset_name:str, instance_id: str):\n     if not os.path.exists(dataset_path):\n         raise ValueError(f\"Dataset file not found at {dataset_path}\")\n \n-    dataset = pd.read_parquet(dataset_path).to_dict(orient=\"records\")\n+    dataset = pd.read_parquet(dataset_path)\n \n-    # Find the instance by instance_id\n-    instance = next((item for item in dataset if item[\"instance_id\"] == instance_id), \n-                    None)\n-    if not instance:\n+    # Find the instance by instance_id \n+    instance = dataset[dataset[\"instance_id\"] == instance_id].iloc[0]\n+\n+    if instance.empty:\n         raise ValueError(f\"Instance {instance_id} not found in dataset\")\n \n     # Pull the repo at the specific commit\n     base_commit = instance[\"base_commit\"]\n-    os.system(f\"git clone {dataset_info['repo_url']} repo\")\n+    # empty the repo directory\n+    shutil.rmtree(\"repo\")\n+    # clone the repo\n+    os.system(f\"git clone https://github.com/{instance['repo']}.git repo\")\n     os.chdir(\"repo\")\n     os.system(f\"git checkout {base_commit}\")\n \n@@ -289,12 +293,16 @@ def main() -> None:\n     skipped_instances = 0\n     for instance_id in instances_ids:\n         setup_instance(dataset_name=dataset_name, instance_id=instance_id)\n+        # change current working directory to the repo directory\n+        os.chdir(\"repo\")\n         experiment_name, skipped_instance = run_ai()\n+        # change current working directory to the parent directory\n+        os.chdir(\"..\")\n         if not skipped_instance:\n             append_to_inferences(inferences_path=os.path.join(\"results\", \n-                                os.path.join(\"subresults\", \"inference.jsonl\"), \\\n+                                os.path.join(\"subresults\", \"inference.jsonl\")), \\\n                                 instance_id=instance_id, \\\n-                                experiment_name=experiment_name))\n+                                experiment_name=experiment_name)\n         else:\n             skipped_instances += 1\n     print(skipped_instances)\ndiff --git a/experimentation/datasets/datasets.yml b/experimentation/datasets/datasets.yml\nindex 8bf08d0..d2f5984 100755\n--- a/experimentation/datasets/datasets.yml\n+++ b/experimentation/datasets/datasets.yml\n@@ -1,3 +1,3 @@\n swe_bench_verified: \n-  url: https://huggingface.co/datasets/princeton-nlp/SWE-bench_Verified\n+  hf_name: princeton-nlp/SWE-bench_Verified\n   local_path: datasets/swe_verified/swe_verified.parquet\n\\ No newline at end of file\ndiff --git a/src/llmops_issue_resolver/cli.py b/src/llmops_issue_resolver/cli.py\nindex 51072c6..37f821e 100644\n--- a/src/llmops_issue_resolver/cli.py\n+++ b/src/llmops_issue_resolver/cli.py\n@@ -36,7 +36,7 @@ def temporary_sys_path(path):\n #     tree\n # All .py modules need to have this line, but with the more general form of the import \n \n-with temporary_sys_path(os.path.abspath(os.join(\"../\", os.path.dirname(__file__)))):\n+with temporary_sys_path(os.path.abspath(os.join(os.path.dirname(__file__), \"../\"))):\n     from llmops_issue_resolver.agent import graph\n \n app = typer.Typer()\n", "model_name_or_path": "placeholder__placeholder__placeholder__placeholder__placeholder"}
{"instance_id": "django__django-11211", "model_patch": "diff --git a/experimentation/SWE-bench b/experimentation/SWE-bench\n--- a/experimentation/SWE-bench\n+++ b/experimentation/SWE-bench\n@@ -1 +1 @@\n-Subproject commit 5f5a7df799663adba4b191eca3d675faf3621fe2\n+Subproject commit 5f5a7df799663adba4b191eca3d675faf3621fe2-dirty\ndiff --git a/experimentation/code/scripts/generate_inferences.py b/experimentation/code/scripts/generate_inferences.py\nindex 0b9a2bb..e9b3a4e 100755\n--- a/experimentation/code/scripts/generate_inferences.py\n+++ b/experimentation/code/scripts/generate_inferences.py\n@@ -3,13 +3,14 @@\n import argparse\n import json\n import os\n+import shutil\n import sys\n from contextlib import contextmanager\n from typing import List\n \n-import datasets as ds\n import pandas as pd\n import yaml\n+from datasets import load_dataset\n from pydantic import ValidationError\n \n \n@@ -89,23 +90,23 @@ def get_instances_ids(dataset_pointer_path: str, dataset_name: str, \\\n     with open(dataset_pointer_path, \"r\") as f:\n         datasets = yaml.safe_load(f)\n \n-    dataset_hf_path = datasets[dataset_name][\"url\"]\n-    print(f\"Dataset hugging face path: {dataset_hf_path}\")\n+    dataset_hf_name= datasets[dataset_name][\"hf_name\"]\n+    print(f\"dataset_hf_name: {dataset_hf_name}\")\n \n-    # Load the dataset from the the huggin face dataset path\n-    dataset = ds.load_dataset(dataset_hf_path, name=dataset_name)\n+    # Load the dataset from the the hugging face dataset path\n+    dataset = load_dataset(dataset_hf_name)\n \n     # Store the dataset locally as parquet\n     dataset_local_path = f\"datasets/{dataset_name}/{dataset_name}.parquet\"\n     os.makedirs(os.path.dirname(dataset_local_path), exist_ok=True)\n-    dataset.to_pandas().to_parquet(dataset_local_path)\n+    dataset[\"test\"].to_pandas().to_parquet(dataset_local_path)\n \n     # Get the instance IDs\n     if random_sampling:\n-        instances_ids = dataset['train'].shuffle(DEFAULT_SEED).select( \\\n+        instances_ids = dataset[\"test\"].shuffle(DEFAULT_SEED).select( \\\n                         range(number_of_instances))['instance_id']\n     else:\n-        instances_ids = dataset['train'].select(range(number_of_instances)) \\\n+        instances_ids = dataset[\"test\"].select(range(number_of_instances)) \\\n                                                 ['instance_id']\n \n     return instances_ids\n@@ -149,7 +150,7 @@ def setup_instance(dataset_name:str, instance_id: str):\n         raise\n \n     # Load dataset information from datasets.yml\n-    with open(\"/datasets/datasets.yml\", \"r\") as f:\n+    with open(\"datasets/datasets.yml\", \"r\") as f:\n         datasets = yaml.safe_load(f)\n \n     dataset_info = datasets.get(dataset_name)\n@@ -162,17 +163,20 @@ def setup_instance(dataset_name:str, instance_id: str):\n     if not os.path.exists(dataset_path):\n         raise ValueError(f\"Dataset file not found at {dataset_path}\")\n \n-    dataset = pd.read_parquet(dataset_path).to_dict(orient=\"records\")\n+    dataset = pd.read_parquet(dataset_path)\n \n-    # Find the instance by instance_id\n-    instance = next((item for item in dataset if item[\"instance_id\"] == instance_id), \n-                    None)\n-    if not instance:\n+    # Find the instance by instance_id \n+    instance = dataset[dataset[\"instance_id\"] == instance_id].iloc[0]\n+\n+    if instance.empty:\n         raise ValueError(f\"Instance {instance_id} not found in dataset\")\n \n     # Pull the repo at the specific commit\n     base_commit = instance[\"base_commit\"]\n-    os.system(f\"git clone {dataset_info['repo_url']} repo\")\n+    # empty the repo directory\n+    shutil.rmtree(\"repo\")\n+    # clone the repo\n+    os.system(f\"git clone https://github.com/{instance['repo']}.git repo\")\n     os.chdir(\"repo\")\n     os.system(f\"git checkout {base_commit}\")\n \n@@ -289,12 +293,16 @@ def main() -> None:\n     skipped_instances = 0\n     for instance_id in instances_ids:\n         setup_instance(dataset_name=dataset_name, instance_id=instance_id)\n+        # change current working directory to the repo directory\n+        os.chdir(\"repo\")\n         experiment_name, skipped_instance = run_ai()\n+        # change current working directory to the parent directory\n+        os.chdir(\"..\")\n         if not skipped_instance:\n             append_to_inferences(inferences_path=os.path.join(\"results\", \n-                                os.path.join(\"subresults\", \"inference.jsonl\"), \\\n+                                os.path.join(\"subresults\", \"inference.jsonl\")), \\\n                                 instance_id=instance_id, \\\n-                                experiment_name=experiment_name))\n+                                experiment_name=experiment_name)\n         else:\n             skipped_instances += 1\n     print(skipped_instances)\ndiff --git a/experimentation/datasets/datasets.yml b/experimentation/datasets/datasets.yml\nindex 8bf08d0..d2f5984 100755\n--- a/experimentation/datasets/datasets.yml\n+++ b/experimentation/datasets/datasets.yml\n@@ -1,3 +1,3 @@\n swe_bench_verified: \n-  url: https://huggingface.co/datasets/princeton-nlp/SWE-bench_Verified\n+  hf_name: princeton-nlp/SWE-bench_Verified\n   local_path: datasets/swe_verified/swe_verified.parquet\n\\ No newline at end of file\ndiff --git a/src/llmops_issue_resolver/cli.py b/src/llmops_issue_resolver/cli.py\nindex 51072c6..37f821e 100644\n--- a/src/llmops_issue_resolver/cli.py\n+++ b/src/llmops_issue_resolver/cli.py\n@@ -36,7 +36,7 @@ def temporary_sys_path(path):\n #     tree\n # All .py modules need to have this line, but with the more general form of the import \n \n-with temporary_sys_path(os.path.abspath(os.join(\"../\", os.path.dirname(__file__)))):\n+with temporary_sys_path(os.path.abspath(os.join(os.path.dirname(__file__), \"../\"))):\n     from llmops_issue_resolver.agent import graph\n \n app = typer.Typer()\n", "model_name_or_path": "placeholder__placeholder__placeholder__placeholder__placeholder"}
